# Multi-Class Synthetic-to-Real Object Detection Challenge  

**Competition:** Co-hosted by **Duality AI** and **3LC**  
**Classes:**  
- `0 â†’ Cheerios`  
- `1 â†’ Soup`  

This repository documents our end-to-end approach for solving the **Synthetic-to-Real Object Detection Challenge**, which aimed to detect objects across both synthetic and real-world domains.  

---

## ğŸ“‚ Dataset  

We used a combination of competition-provided images and synthetic data generated under two scenarios:  

1. **Competition Dataset**
   - Synthetic data provided by the organizers.
   - Real-life images provided by the organizers.

2. **Synthetic Data generated by**  
   - **Falcon Automated Data-Generation Scenario**  
   - **Controlled Data-Generation Scenario**  

All images were grouped together and then split into:  

- **5 Folds** â†’ for cross-validation training.  
- **Validation Folder (`val`)** â†’ contained only real-life competition images, reserved strictly for validation purposes.  
- **Test Set** â†’ consisted of real-life competition images for final inference.  

This structure ensured robust training on diverse synthetic images while keeping the validation/test sets real-world for domain generalization.  

- **Dataset Set Link**: [Dataset](https://www.kaggle.com/datasets/younesbenalia/falcon-multiclass-dataset)

---

## ğŸ‹ï¸ Training  

We used **YOLO11x** from [Ultralytics](https://github.com/ultralytics/ultralytics) as our detection backbone.  
Our strategy was **5-fold cross-validation**:  

- For each fold:  
  - **Training** â†’ 4 folds of synthetic data.  
  - **Validation** â†’ 1 fold + the `val` folder (real-world images).  

### Training Configuration  

```python
model = YOLO("yolo11x.pt")

for i in range(1, 6):  
    model.train(
        data=f"data{i}.yaml",     # Data config for each fold
        epochs=40,                
        batch=8,                   
        imgsz=1056,
        patience=40,               
        optimizer='SGD',
        momentum=0.937,          
        lr0=0.0002,                
        weight_decay=0.0005,       
        cos_lr=True,               
        save_period=5,             
        workers=8,
        device=[0, 1],

        # Augmentations
        close_mosaic=0,
        hsv_h=0.015,
        hsv_s=0.7,
        hsv_v=0.4,
        flipud=0,
        fliplr=0.5,
        translate=0.1,
        scale=0.5,
        shear=0.01,
        mixup=0.1,

        # Warmup & Precision
        warmup_epochs=3,
        cache=True,
        amp=True,
    )
```
- **Metric Target**:
  - Optimized for **mAP@0.5 IoU**

- **ğŸ“Š Kaggle notebook**: [notebook](https://www.kaggle.com/code/younesbenalia/falcon-multiclass-yolo-train)

---

# Model Highlights, Inference, and Results  

---

## ğŸ”‘ Key Highlights  

- **SGD optimizer** with cosine LR scheduling.  
- **Aggressive augmentations** to encourage robustness.  
- **Mixed Precision (AMP)** for efficiency.  
- Training across **2 GPUs** (`device=[0,1]`).  

---

## ğŸ” Inference  

For each trained model, we performed inference on the test set under **6 different image sizes**:  640, 800, 1056, 1376, 1600, 1920
- **Model Selection**:
  - Used the checkpoint that performed best on the validation set: `best.pt`


### Settings Used  

- `conf = 0`  
- nms `iou = 0.4`  
- **Test Time Augmentation (TTA) enabled**  

Each inference generated predictions saved as **CSV files** in accordance with the competition submission format.  

- **ğŸ“Š Kaggle notebook**: [notebook](https://www.kaggle.com/code/younesbenalia/falcon-multiclass-yolo-inference)

---

## ğŸ¤ Ensemble  

To combine predictions, we used **Weighted Box Fusion (WBF)** with the following parameters:  

- **Invalid box filtering**
- `iou_threshold = 0.55`  
- `skip_box_threshold = 0`  

All predictions from **5 models Ã— 6 image sizes** were ensembled into a single CSV file.  

---

## ğŸ“¤ Submission

- **Prediction Validation**:
  - Strict format checking to comply with submission requirements.
  - Automatic handling for images with **no detections**.

- **Submission Format**:
  - Final predictions are converted into the required format.
  - **Output File**: `submission_wbf_0.55_0.csv`

- **ğŸ“Š Kaggle notebook**: [notebook](https://www.kaggle.com/code/younesbenalia/falcon-multiclass-apply-wbf)

---

## ğŸ“Š Results  

- **Public Leaderboard Score:** `0.991 mAP@50`  
- **Private Leaderboard Score:** `0.996 mAP@50`

---

## â™»ï¸ Reproducibility

- Fixed random seeds across the entire training pipeline.
- **Hardware** used: 2x `NVIDIA Tesla T4 GPU`.

---

## ğŸ“‡ Contact Information

If you'd like to connect, collaborate, or ask questions, feel free to reach out:

- **ğŸ‘¨â€ğŸ’» Author**: Younes Benalia  
- **ğŸ“§ Email**: [younes.benalia.dz@gmail.com]  
- **ğŸ”— LinkedIn**: [younesbenalia](https://www.linkedin.com/in/younesbenalia)  
- **ğŸ“Š Kaggle**: [younesbenalia](https://www.kaggle.com/younesbenalia)  
- **ğŸ™ GitHub**: [younesBenalia](https://github.com/younesBenalia)
- **ğŸ™ Huggingface**: [younesbenalia](https://huggingface.co/younesbenalia)  
---

